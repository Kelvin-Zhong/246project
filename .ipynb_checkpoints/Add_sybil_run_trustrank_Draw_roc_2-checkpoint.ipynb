{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PageRank function\n",
    "import networkx as nx\n",
    "def pagerank_scipy(G, alpha=0.85, personalization=None,\n",
    "                   max_iter=100, tol=1.0e-6, weight='weight',\n",
    "                   dangling=None):\n",
    "\n",
    "    import scipy.sparse\n",
    "\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}\n",
    "\n",
    "    nodelist = G.nodes()\n",
    "    M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, weight=weight,\n",
    "                                  dtype=float)\n",
    "    S = scipy.array(M.sum(axis=1)).flatten()\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "    Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')\n",
    "    M = Q * M\n",
    "\n",
    "    # initial vector\n",
    "    x = scipy.repeat(1.0 / N, N)\n",
    "\n",
    "    # Personalization vector\n",
    "    if personalization is None:\n",
    "        p = scipy.repeat(1.0 / N, N)\n",
    "    else:\n",
    "        missing = set(nodelist) - set(personalization)\n",
    "        if missing:\n",
    "            raise NetworkXError('Personalization vector dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        p = scipy.array([personalization[n] for n in nodelist],\n",
    "                        dtype=float)\n",
    "        p = p / p.sum()\n",
    "\n",
    "    # Dangling nodes\n",
    "    if dangling is None:\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "        missing = set(nodelist) - set(dangling)\n",
    "        if missing:\n",
    "            raise NetworkXError('Dangling node dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        # Convert the dangling dictionary into an array in nodelist order\n",
    "        dangling_weights = scipy.array([dangling[n] for n in nodelist],\n",
    "                                       dtype=float)\n",
    "        dangling_weights /= dangling_weights.sum()\n",
    "    is_dangling = scipy.where(S == 0)[0]\n",
    "\n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \\\n",
    "            (1 - alpha) * p\n",
    "        # check convergence, l1 norm\n",
    "        err = scipy.absolute(x - xlast).sum()\n",
    "        if err < N * tol:\n",
    "            return dict(zip(nodelist, map(float, x)))\n",
    "    raise NetworkXError('pagerank_scipy: power iteration failed to converge '\n",
    "                        'in %d iterations.' % max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "def add_sybils(DG,d,a):\n",
    "    \n",
    "    true_nodes = DG.nodes()\n",
    "    node_number = len(true_nodes)\n",
    "    DG2 = copy.deepcopy(DG)\n",
    "    true_nodes_number = len(true_nodes)\n",
    "    start = max(DG.nodes()) + 1\n",
    "    Sybil_number = int(node_number * 0.3)\n",
    "    new_node = []\n",
    "    \n",
    "    for i in range(Sybil_number):\n",
    "        DG2.add_node(start+i)\n",
    "        new_node.append(start+i)\n",
    "    node_number2 = len(DG2.nodes())\n",
    "    \n",
    "    print node_number\n",
    "    print node_number2\n",
    "\n",
    "    for node in new_node:\n",
    "        for i in range(d):\n",
    "            to = random.randint(start, start+Sybil_number)\n",
    "            DG2.add_edge(node, to)\n",
    "            DG2.add_edge(to, node)\n",
    "\n",
    "    for i in range(a):\n",
    "        from_one = true_nodes[random.randint(0, true_nodes_number-1)]\n",
    "        to_other = new_node[random.randint(0, Sybil_number-1)]\n",
    "        DG2.add_edge(from_one, to_other)\n",
    "        DG2.add_edge(to_other, from_one)\n",
    "        \n",
    "    return DG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_personalized_vector(DG2,TrustSet):\n",
    "    NumberofTrust = len(TrustSet)\n",
    "    Personalized_vector= DG2.in_degree()\n",
    "    for key in Personalized_vector:\n",
    "        if key in TrustSet:\n",
    "            Personalized_vector[key] = 0\n",
    "        else:\n",
    "            Personalized_vector[key] = 1.0 / NumberofTrust\n",
    "    return Personalized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_trust_seeds(DG,numberofseed):\n",
    "    l = []\n",
    "    for n in DG:\n",
    "        numfollower = len(DG.in_edges(n))\n",
    "        #print G.in_edges(n), numfollower\n",
    "        l.append([n, numfollower])\n",
    "\n",
    "    l = sorted(l, key=lambda s: s[1], reverse=True)\n",
    "\n",
    "    #Set the top 100 nodes as trust\n",
    "    t = []\n",
    "    for i in range(numberofseed):\n",
    "        t.append(l[i][0])\n",
    "\n",
    "    Personalized_vector= DG.in_degree()\n",
    "    TrustSet = set(t)\n",
    "    return TrustSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y_with_pagerank(DG2,true_nodes):\n",
    "    #After getting the pagerank value we will go to find the ROC\n",
    "    ### Construct y and result_prob\n",
    "    nodes = DG2.nodes()\n",
    "    y = []\n",
    "    result_prob = []\n",
    "    index = []\n",
    "    for i in nodes:\n",
    "        if i in true_nodes:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "        result_prob.append(result[i])\n",
    "        index.append(i)\n",
    "    return y,result_prob,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthreshold = 0.001\\ntrue_nodes_set = set(true_nodes)\\nTN = 0\\nTP = 0\\nFN = 0\\nFP = 0\\n\\nfor i in range(len(result_prob)):\\n    if result_prob[i] < threshold:\\n        if index[i] in true_nodes_set:\\n            FP += 1\\n        else:\\n            TN += 1\\n    else:\\n        if index[i] in true_nodes_set:\\n            TP += 1\\n        else:\\n            FN += 1\\nTPR = TP * 1.0 / (TP + FN)\\nFPR = FP * 1.0 / (FP + TN)\\nprint TPR,FPR\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "predict_sybil = []\n",
    "false_negative_nodes = []\n",
    "true_negative_nodes = []\n",
    "\n",
    "for i in range(len(result_prob)):\n",
    "    if result_prob[i] < 0.001:\n",
    "        predict_sybil.append(index[i])\n",
    "        \n",
    "        if index[i] in true_nodes_set:\n",
    "            true_negative_nodes.append(index[i])\n",
    "        else:\n",
    "            false_negative_nodes.append(index[i])\n",
    "\n",
    "print len(true_negative_nodes),len(false_negative_nodes),len()\n",
    "'''\n",
    "\n",
    "def compute_TPR_FPR(true_nodes_set,threshold,result_prob):\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    \n",
    "    for i in range(len(result_prob)):\n",
    "        if result_prob[i] < threshold:\n",
    "            if index[i] in true_nodes_set:\n",
    "                FP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if index[i] in true_nodes_set:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "    TPR = TP * 1.0 / (TP + FN)\n",
    "    FPR = FP * 1.0 / (FP + TN)\n",
    "    print TPR,FPR\n",
    "    return TPR,FPR\n",
    "    \n",
    "'''\n",
    "threshold = 0.001\n",
    "true_nodes_set = set(true_nodes)\n",
    "TN = 0\n",
    "TP = 0\n",
    "FN = 0\n",
    "FP = 0\n",
    "\n",
    "for i in range(len(result_prob)):\n",
    "    if result_prob[i] < threshold:\n",
    "        if index[i] in true_nodes_set:\n",
    "            FP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    else:\n",
    "        if index[i] in true_nodes_set:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FN += 1\n",
    "TPR = TP * 1.0 / (TP + FN)\n",
    "FPR = FP * 1.0 / (FP + TN)\n",
    "print TPR,FPR\n",
    "'''\n",
    "#True Positive Rate: Recall: (TP) / (TP + FN)\n",
    "\n",
    "#False Positive Rate: FP / (FP + TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter the FPR and TPR to ensure no negative slew\n",
    "def filter_FPR_TPR(TPR_array,FPR_array):\n",
    "    TPR_array2 = [0.0]\n",
    "    FPR_array2 = [0.0]\n",
    "    for i in range(1,len(TPR_array)):\n",
    "        if TPR_array[i] >= TPR_array[i-1]:\n",
    "            TPR_array2.append(TPR_array[i])\n",
    "            FPR_array2.append(FPR_array[i])\n",
    "    return TPR_array2,FPR_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the original true graph from file\n",
    "import networkx as nx\n",
    "import pickle\n",
    "DG = pickle.load( open( '246_source/retweet_network_graph.p', \"rb\" ) )\n",
    "\n",
    "true_nodes = set(DG.nodes())\n",
    "\n",
    "numberofseed = 100\n",
    "TrustSet = initialize_trust_seeds(DG,numberofseed)\n",
    "\n",
    "TPR_array = [0.0]\n",
    "FPR_array = [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256491\n",
      "333438\n"
     ]
    }
   ],
   "source": [
    "#run trust_rank\n",
    "d = 3\n",
    "a = 3\n",
    "DG2 = add_sybils(DG,d,a)\n",
    "Personalized_vector = get_personalized_vector(DG2,TrustSet)\n",
    "result = pagerank_scipy(DG2,personalization=Personalized_vector)\n",
    "y,result_prob,index = get_y_with_pagerank(DG2,true_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(DG2.nodes())\n",
    "max_abs_prob = max(result_prob) * n * 1.0\n",
    "for i in xrange(len(result_prob)):\n",
    "    result_prob[i] = result_prob[i] * n / max_abs_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.768994296007\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.01\n",
    "TPR,FPR = compute_TPR_FPR(true_nodes,threshold,result_prob)\n",
    "TPR_array.append(TPR)\n",
    "FPR_array.append(FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TPR_array.append(1.0)\n",
    "FPR_array.append(1.0)\n",
    "TPR_array,FPR_array = filter_FPR_TPR(TPR_array,FPR_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "roc_auc = auc(FPR_array, TPR_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, [0.0, 1.0], [0.0, 1.0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc,TPR_array,FPR_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.036073\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Use package to draw and compute the ROC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "#### Here probas_[:,1] is a numpy array of probabilities\n",
    "fpr, tpr, thresholds = roc_curve(y, result_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print \"Area under the ROC curve : %f\" % roc_auc\n",
    "\n",
    "# Plot ROC curve\n",
    "pl.clf()\n",
    "pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('Receiver operating characteristic example')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve : 0.793881\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Make it a binary classification problem by removing the third class\n",
    "X, y = X[y != 2], y[y != 2]\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X, y = shuffle(X, y, random_state=random_state)\n",
    "half = int(n_samples / 2)\n",
    "X_train, X_test = X[:half], X[half:]\n",
    "y_train, y_test = y[:half], y[half:]\n",
    "\n",
    "# Run classifier\n",
    "classifier = svm.SVC(kernel='linear', probability=True)\n",
    "probas_ = classifier.fit(X_train, y_train).predict_proba(X_test)\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print \"Area under the ROC curve : %f\" % roc_auc\n",
    "\n",
    "# Plot ROC curve\n",
    "pl.clf()\n",
    "pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "pl.plot([0, 1], [0, 1], 'k--')\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('Receiver operating characteristic example')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
